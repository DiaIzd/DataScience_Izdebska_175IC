{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Untitled22.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyN7Mupwfpvm+lImFhuVvlpp",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/DiaIzd/DataScience_Izdebska_175IC/blob/main/wyk%C5%82ad5.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "16VIBIO3qlnV"
      },
      "source": [
        "# !pip install spacy --upgrade\r\n",
        "# !spacy download pl_core_news_sm\r\n",
        "# !pip install textacy --upgrade\r\n",
        "# import spacy\r\n",
        "# import pl_core_news_sm\r\n",
        "# import re\r\n",
        "# from spacy.tokenizer import Tokenizer\r\n",
        "# from collections import Counter\r\n",
        "# from spacy import displacy\r\n",
        "# from spacy.matcher import Matcher\r\n",
        "# import textacy\r\n",
        "# import warnings\r\n",
        "\r\n",
        "warnings.filterwarnings(\"ignore\")\r\n",
        "\r\n",
        "nlp = pl_core_news_sm.load()\r\n",
        "doc = nlp(\"To jest zdanie.\")\r\n",
        "print([(w.text, w.pos_) for w in doc])\r\n",
        "\r\n",
        "#CZYTANIE STRINGA\r\n",
        "text = ('This is the last semester')\r\n",
        "result = nlp(text)\r\n",
        "print ([token.text for token in result])\r\n",
        "\r\n",
        "#CZYTANIE PLIKU\r\n",
        "blackpink_file = 'blackpink.txt'\r\n",
        "blackpink_file_text = open(blackpink_file).read()\r\n",
        "blackpink_file_doc = nlp(blackpink_file_text)\r\n",
        "print ([token.text for token in blackpink_file_doc])\r\n",
        "\r\n",
        "#WYKRYWANIE ZDAŃ\r\n",
        "about_doc = ('Jak bardzo Chiny są egzotycznym krajem dla Europejczyków, przekonać się można na każdym kroku. Wdychając po raz pierwszy powietrze, można zauważyć, że ono zupełnie inaczej pachnie i nie przypomina znajomych nam aromatów. Już przy pierwszym wdechu można poczuć delikatny zapach chińskiej kuchni, przesiąkniętej zupełnie innymi, orientalnymi przyprawami. Być może tak się dzieje, ponieważ Chińczycy kochają jedzenie, na każdym kroku, na każdej ulicy można natknąć się na miejsca, w których się je przygotowuje. To może być ogromna restauracja, ale może też być jeden plastikowy stolik na chodniku, a obok jeden garnek postawiony na prowizorycznym palenisku. Takie są Chiny. Atrakcje turystyczne wyzierają z każdego zakątka i nie sposób znudzić się nimi.')\r\n",
        "text_doc = nlp(about_doc)\r\n",
        "sentences = list(text_doc.sents)\r\n",
        "len(sentences)\r\n",
        "\r\n",
        "for sentence in sentences:\r\n",
        "  print (sentence)\r\n",
        "\r\n",
        "#WYKRYWANIE ZDAŃ Z NIESTANDARDOWYMI OGRANICZNIKAMI.\r\n",
        "def set_custom_boundaries(doc):\r\n",
        "# Adds support to use `...` as the delimiter for sentence detection\r\n",
        " for token in doc[:-1] :\r\n",
        "  if token.text == '...':\r\n",
        "   doc[token.i+1].is_sent_start = True\r\n",
        "  return doc\r\n",
        "\r\n",
        "ellipsis_text = ('Gus, can you, ... never mind, I forgot what I was saying. So, do you think we should ...')\r\n",
        "#Załadowanie nowej instancji modelu\r\n",
        "custom_nlp = spacy.load('pl_core_news_sm')\r\n",
        "custom_nlp.add_pipe(set_custom_boundaries, before='parser')\r\n",
        "custom_ellipsis_doc = custom_nlp(ellipsis_text)\r\n",
        "custom_ellipsis_sentences = list(custom_ellipsis_doc.sents)\r\n",
        "for sentence in custom_ellipsis_sentences:\r\n",
        "  print(sentence)\r\n",
        "\r\n",
        "#Wykrywanie zdań bez dostosowywania\r\n",
        "ellipsis_doc = nlp(ellipsis_text)\r\n",
        "ellipsis_sentences = list(ellipsis_doc.sents)\r\n",
        "for sentence in ellipsis_sentences:\r\n",
        "  print(sentence)\r\n",
        "\r\n",
        "#TOKENIZACJA TEKSTU\r\n",
        "for token in text_doc:\r\n",
        "  print (token, token.idx)\r\n",
        "#DOSTĘP DO ATRYBUTÓW W TOKENIE\r\n",
        "for token in text_doc:\r\n",
        " print (token, token.idx, token.text_with_ws,\r\n",
        "  token.is_alpha, token.is_punct, token.is_space,\r\n",
        "  token.shape_, token.is_stop)\r\n",
        "\r\n",
        "#DOSTOSOWANIE TOKENU\r\n",
        "\r\n",
        "custom_nlp = spacy.load('pl_core_news_sm')\r\n",
        "prefix_re = spacy.util.compile_prefix_regex(custom_nlp.Defaults.prefixes)\r\n",
        "suffix_re = spacy.util.compile_suffix_regex(custom_nlp.Defaults.suffixes)\r\n",
        "infix_re = re.compile(r'''[-~]''')\r\n",
        "def customize_tokenizer(nlp):\r\n",
        "  # Adds support to use `-` as the delimiter for tokenization\r\n",
        "  return Tokenizer(nlp.vocab, prefix_search=prefix_re.search,\r\n",
        "     suffix_search=suffix_re.search,\r\n",
        "     infix_finditer=infix_re.finditer,\r\n",
        "     token_match=None\r\n",
        "     )\r\n",
        "\r\n",
        "custom_nlp.tokenizer = customize_tokenizer(custom_nlp)\r\n",
        "custom_tokenizer_text_doc = custom_nlp(about_doc)\r\n",
        "print([token.text for token in custom_tokenizer_text_doc])\r\n",
        "\r\n",
        "#STOP WORDS\r\n",
        "spacy_stopwords = spacy.lang.pl.stop_words.STOP_WORDS\r\n",
        "len(spacy_stopwords)\r\n",
        "326\r\n",
        "for stop_word in list(spacy_stopwords)[:10]:\r\n",
        "  print(stop_word)\r\n",
        "#POMINIĘCIE STOP WORDS W TEKŚCIE WEJŚCIOWYM\r\n",
        "for token in text_doc:\r\n",
        "  if not token.is_stop:\r\n",
        "   print (token)\r\n",
        "#LEMATYZACJA\r\n",
        "conference_help_text = ('Gus is helping organize a developer'\r\n",
        "   'conference on Applications of Natural Language'\r\n",
        "   ' Processing. He keeps organizing local Python meetups'\r\n",
        "   ' and several internal talks at his workplace.')\r\n",
        "conference_help_doc = nlp(conference_help_text)\r\n",
        "for token in conference_help_doc:\r\n",
        "   print (token, token.lemma_)\r\n",
        "#CZĘSTOTLIWOŚĆ SŁÓW\r\n",
        "complete_text = ('Jak bardzo Chiny są egzotycznym krajem dla Europejczyków, przekonać się można na każdym kroku. Wdychając po raz pierwszy powietrze, można zauważyć, że ono zupełnie inaczej pachnie i nie przypomina znajomych nam aromatów. Już przy pierwszym wdechu można poczuć delikatny zapach chińskiej kuchni, przesiąkniętej zupełnie innymi, orientalnymi przyprawami. Być może tak się dzieje, ponieważ Chińczycy kochają jedzenie, na każdym kroku, na każdej ulicy można natknąć się na miejsca, w których się je przygotowuje. To może być ogromna restauracja, ale może też być jeden plastikowy stolik na chodniku, a obok jeden garnek postawiony na prowizorycznym palenisku. Takie są Chiny. Atrakcje turystyczne wyzierają z każdego zakątka i nie sposób znudzić się nimi.')\r\n",
        "\r\n",
        "complete_doc = nlp(complete_text)\r\n",
        "#Usunięcie słów pomijanych i symboli interpunkcyjnych\r\n",
        "words = [token.text for token in complete_doc\r\n",
        "     if not token.is_stop and not token.is_punct]\r\n",
        "word_freq = Counter(words)\r\n",
        "#5 powszechnie występujących słów wraz z ich częstotliwościami\r\n",
        "common_words = word_freq.most_common(5)\r\n",
        "print (common_words)\r\n",
        "# Wyrazy unikatowe\r\n",
        "unique_words = [word for (word, freq) in word_freq.items() if freq == 1]\r\n",
        "print (unique_words)\r\n",
        "#ROZPOZNANIE CZĘŚCI MOWY\r\n",
        "for token in text_doc:\r\n",
        "  print (token, token.tag_, token.pos_, spacy.explain(token.tag_))\r\n",
        "#WYZNACZENIE SŁÓW PO DANEJ KATEGORII\r\n",
        "nouns = []\r\n",
        "adjectives = []\r\n",
        "for token in text_doc:\r\n",
        "  if token.pos_ == 'NOUN':\r\n",
        "       nouns.append(token)\r\n",
        "  if token.pos_ == 'ADJ':\r\n",
        "       adjectives.append(token)\r\n",
        "\r\n",
        "nouns\r\n",
        "\r\n",
        "adjectives\r\n",
        "# #WISUALIZACJA UŻYWAJĄC DISPLACY\r\n",
        "\r\n",
        "# about_interest_text = ('He is interested in learning'\r\n",
        "#   ' Natural Language Processing.')\r\n",
        "# about_interest_doc = nlp(about_interest_text)\r\n",
        "# displacy.serve(about_interest_doc, style='dep')\r\n",
        "\r\n",
        "#FUNKCJE PRZETWARZAJĄCE\r\n",
        "def is_token_allowed(token):\r\n",
        "  if (not token or not token.string.strip() or\r\n",
        "     token.is_stop or token.is_punct):\r\n",
        "     return False\r\n",
        "  return True\r\n",
        "\r\n",
        "def preprocess_token(token):\r\n",
        "\r\n",
        "   return token.lemma_.strip().lower()\r\n",
        "\r\n",
        "complete_filtered_tokens = [preprocess_token(token)\r\n",
        "  for token in complete_doc if is_token_allowed(token)]\r\n",
        "complete_filtered_tokens\r\n",
        "#DOPASOWANIE OPARTE NA REGUŁACH IMIĘ I NAZWISKO\r\n",
        "matcher = Matcher(nlp.vocab)\r\n",
        "def extract_full_name(nlp_doc):\r\n",
        " pattern = [{'POS':'PROPN'}, {'POS': 'PROPN'}]\r\n",
        " matcher.add('FULL_NAME ', None, pattern)\r\n",
        " matches = matcher(nlp_doc)\r\n",
        " for match_id, start, end in matches:\r\n",
        "  span = nlp_doc[start:end]\r\n",
        "  return span.text\r\n",
        "extract_full_name(text_doc)\r\n",
        "new_text_extract=\"Diana Izdebska\"\r\n",
        "new_text_extract_doc=nlp(new_text_extract)\r\n",
        "matcher = Matcher(nlp.vocab)\r\n",
        "print(extract_full_name(new_text_extract_doc))\r\n",
        "#DOPASOWANIE OPARTE NA REGUŁACH NUMER TELEFONU\r\n",
        "matcher = Matcher(nlp.vocab)\r\n",
        "conference_org_text = ('111222333')\r\n",
        "def extract_phone_number(nlp_doc):\r\n",
        " pattern = [{'ORTH': '('}, {'SHAPE': 'ddd'},\r\n",
        "            {'ORTH': ')'}, {'SHAPE': 'ddd'},\r\n",
        "            {'ORTH': '-', 'OP': '?'},\r\n",
        "            {'SHAPE': 'ddd'}]\r\n",
        " matcher.add('PHONE_NUMBER', None, pattern)\r\n",
        " matches = matcher(nlp_doc)\r\n",
        " for match_id, start, end in matches:\r\n",
        "  span = nlp_doc[start:end]\r\n",
        "  return span.text\r\n",
        "\r\n",
        "conference_org_doc = nlp(conference_org_text)\r\n",
        "extract_phone_number(conference_org_doc)\r\n",
        "\r\n",
        "# ANALIZA ZALEŻNOŚCI PRZY UŻYCIU SPACY\r\n",
        "piano_text = 'Gus is learning piano'\r\n",
        "piano_doc = nlp(piano_text)\r\n",
        "for token in piano_doc:\r\n",
        " print (token.text, token.tag_, token.head.text, token.dep_)\r\n",
        "#PORUSZANIE SIĘ PO DRZEWIE I PODDRZEWIE\r\n",
        "one_line_about_doc = ('Gus Proto is a Python developer'\r\n",
        "                       ' currently working for a London-based Fintech company')\r\n",
        "one_line_about_doc = nlp(one_line_about_doc)\r\n",
        "# Extract children of `developer`\r\n",
        "print([token.text for token in one_line_about_doc[5].children])\r\n",
        "\r\n",
        "# Extract previous neighboring node of `developer`\r\n",
        "print (one_line_about_doc[5].nbor(-1))\r\n",
        "\r\n",
        "# Extract next neighboring node of `developer`\r\n",
        "print (one_line_about_doc[5].nbor())\r\n",
        "\r\n",
        "# Extract all tokens on the left of `developer`\r\n",
        "print([token.text for token in one_line_about_doc[5].lefts])\r\n",
        "\r\n",
        "# Extract tokens on the right of `developer`\r\n",
        "print([token.text for token in one_line_about_doc[5].rights])\r\n",
        "\r\n",
        "# Print subtree of `developer`\r\n",
        "print (list(one_line_about_doc[5].subtree))\r\n",
        "#Wykrywanie fraz rzeczownikowych\r\n",
        "conference_text = ('There is a developer conference'\r\n",
        "                  ' happening on 21 July 2019 in London.')\r\n",
        "conference_doc = nlp(conference_text)\r\n",
        "# Wyodrębnienie fraz rzeczownikowych\r\n",
        "for chunk in conference_doc.noun_chunks:\r\n",
        " print (chunk)\r\n",
        "\r\n",
        "#Wykrywanie fraz czasownika\r\n",
        "about_talk_text = ('The talk will introduce reader about Use'\r\n",
        "                   ' cases of Natural Language Processing in'\r\n",
        "                   ' Fintech')\r\n",
        "pattern = [{\"POS\": \"VERB\", \"OP\": \"*\"},{\"POS\": \"ADV\", \"OP\": \"*\"},{\"POS\": \"VERB\", \"OP\": \"+\"},{\"POS\": \"PART\", \"OP\": \"*\"}]\r\n",
        "about_talk_doc = textacy.make_spacy_doc(about_talk_text, nlp)\r\n",
        "verb_phrases = textacy.extract.matches(about_talk_doc, pattern)\r\n",
        "for chunk in verb_phrases:\r\n",
        "    print(chunk.text)\r\n",
        "\r\n",
        "for chunk in about_talk_doc.noun_chunks:\r\n",
        "    print (chunk)\r\n",
        "\r\n",
        "#Wyodrębnienie wyrażenia rzeczownikowego, aby wyjaśnić, o jakie rzeczowniki chodzi\r\n",
        "for chunk in about_talk_doc.noun_chunks:\r\n",
        " print (chunk)\r\n",
        "#Rozpoznawanie nazwanych jednostek\r\n",
        "piano_class_text = ('Great Piano Academy is situated'\r\n",
        "                    ' in Mayfair or the City of London and has'\r\n",
        "                    ' world-class piano instructors.')\r\n",
        "piano_class_doc = nlp(piano_class_text)\r\n",
        "for ent in piano_class_doc.ents:\r\n",
        " print(ent.text, ent.start_char, ent.end_char, ent.label_, spacy.explain(ent.label_))"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}